{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab6251f4",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50863705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/yashv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/yashv/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/yashv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# To perform basic text preprocessing\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# To save files\n",
    "import pickle\n",
    "\n",
    "# For tracking progress in loops\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For BOW and TFIDF\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab366375",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "270c80f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(words: list) -> list:\n",
    "    \"\"\"\n",
    "    Takes a list of words, and returns them after lemmatizing each word.\n",
    "    \n",
    "    INPUTS:\n",
    "        :words (list): List of input words.\n",
    "    \n",
    "    OUTPUTS:\n",
    "        :(list): List of words after lemmatizing each input word.\n",
    "    \"\"\"\n",
    "    return [WordNetLemmatizer().lemmatize(word, 'v') for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b041706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bag_of_words(sentiment_words: list, docs: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get Bag-of-words representation for the input documents.\n",
    "    \n",
    "    INPUTS:\n",
    "        :sentiment_words (list): List of words characterizing a sentiment.\n",
    "        :docs (list): List of strings. Each string is a different 10K/Q form.\n",
    "    \n",
    "    OUTPUTS:\n",
    "        :bag_of_words (np.ndarray): shape - (#docs, #<unique words in all docs>)\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer(vocabulary=sentiment_words)\n",
    "    vectors = vec.fit_transform(docs)\n",
    "    words_list = vec.get_feature_names()\n",
    "    bag_of_words = np.zeros([len(docs), len(words_list)])\n",
    "    \n",
    "    for doc_idx in range(len(docs)):\n",
    "        bag_of_words[doc_idx] = vectors[doc_idx].toarray()[0]\n",
    "\n",
    "    return bag_of_words.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc617ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(sentiment_words: list, docs: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get TF-IDF representation for the input documents.\n",
    "    \n",
    "    INPUTS:\n",
    "        :sentiment_words (list): List of words characterizing a sentiment.\n",
    "        :docs (list): List of strings. Each string is a different 10K/Q form.\n",
    "    \n",
    "    OUTPUTS:\n",
    "        :tfidf (np.ndarray): shape - (#docs, #<unique words in all docs>)\n",
    "    \"\"\"\n",
    "    vec = TfidfVectorizer(vocabulary=sentiment_words)\n",
    "    tfidf = vec.fit_transform(docs)\n",
    "    \n",
    "    return tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed7a0f",
   "metadata": {},
   "source": [
    "# Preparing Sentiment Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfb3385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = [\"negative\", \"positive\", \"uncertainty\", \n",
    "              \"litigious\", \"strong_modal\", \"weak_modal\", \n",
    "              \"constraining\", \"complexity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a12f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66289, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>seq_num</th>\n",
       "      <th>word count</th>\n",
       "      <th>word proportion</th>\n",
       "      <th>average proportion</th>\n",
       "      <th>std dev</th>\n",
       "      <th>doc count</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>uncertainty</th>\n",
       "      <th>litigious</th>\n",
       "      <th>strong_modal</th>\n",
       "      <th>weak_modal</th>\n",
       "      <th>constraining</th>\n",
       "      <th>complexity</th>\n",
       "      <th>syllables</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>1</td>\n",
       "      <td>312</td>\n",
       "      <td>1.422050e-08</td>\n",
       "      <td>1.335201e-08</td>\n",
       "      <td>3.700747e-06</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aardvarks</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.367356e-10</td>\n",
       "      <td>8.882163e-12</td>\n",
       "      <td>9.362849e-09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abaci</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4.102067e-10</td>\n",
       "      <td>1.200533e-10</td>\n",
       "      <td>5.359747e-08</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aback</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>6.836779e-10</td>\n",
       "      <td>4.080549e-10</td>\n",
       "      <td>1.406914e-07</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abacus</td>\n",
       "      <td>5</td>\n",
       "      <td>8009</td>\n",
       "      <td>3.650384e-07</td>\n",
       "      <td>3.798698e-07</td>\n",
       "      <td>3.523914e-05</td>\n",
       "      <td>1058</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12of12inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  seq_num  word count  word proportion  average proportion  \\\n",
       "0   aardvark        1         312     1.422050e-08        1.335201e-08   \n",
       "1  aardvarks        2           3     1.367356e-10        8.882163e-12   \n",
       "2      abaci        3           9     4.102067e-10        1.200533e-10   \n",
       "3      aback        4          15     6.836779e-10        4.080549e-10   \n",
       "4     abacus        5        8009     3.650384e-07        3.798698e-07   \n",
       "\n",
       "        std dev  doc count  negative  positive  uncertainty  litigious  \\\n",
       "0  3.700747e-06         96         0         0            0          0   \n",
       "1  9.362849e-09          1         0         0            0          0   \n",
       "2  5.359747e-08          7         0         0            0          0   \n",
       "3  1.406914e-07         14         0         0            0          0   \n",
       "4  3.523914e-05       1058         0         0            0          0   \n",
       "\n",
       "   strong_modal  weak_modal  constraining  complexity  syllables     source  \n",
       "0             0           0             0           0          2  12of12inf  \n",
       "1             0           0             0           0          2  12of12inf  \n",
       "2             0           0             0           0          3  12of12inf  \n",
       "3             0           0             0           0          2  12of12inf  \n",
       "4             0           0             0           0          3  12of12inf  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df = pd.read_csv(\"LoughranMcDonald_MasterDictionary_2020.csv\")\n",
    "sentiment_df.columns = [column.lower() for column in sentiment_df.columns]\n",
    "sentiment_df = sentiment_df[sentiment_df['word'].notna()]\n",
    "\n",
    "sentiment_df['word'] = lemmatize_words(sentiment_df['word'].str.lower())\n",
    "sentiment_df = sentiment_df.drop_duplicates('word')\n",
    "print(sentiment_df.shape)\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dc9519",
   "metadata": {},
   "source": [
    "# Loading 10K and 10Q dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f7bcb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ten_q_forms.pickle', 'rb') as handle:\n",
    "    ten_q_forms = pickle.load(handle)\n",
    "\n",
    "with open('ten_q_forms_dates.pickle', 'rb') as handle:\n",
    "    ten_q_forms_dates = pickle.load(handle)\n",
    "\n",
    "with open('ten_k_forms.pickle', 'rb') as handle:\n",
    "    ten_k_forms = pickle.load(handle)\n",
    "\n",
    "with open('ten_k_forms_dates.pickle', 'rb') as handle:\n",
    "    ten_k_forms_dates = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b624d3f",
   "metadata": {},
   "source": [
    "# Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c42fb9c",
   "metadata": {},
   "source": [
    "## 10Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45c966d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]/opt/anaconda3/envs/stock/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "100%|█████████████████████████████████████████████| 5/5 [02:49<00:00, 33.87s/it]\n"
     ]
    }
   ],
   "source": [
    "sentiment_bow_ten_qs = {}\n",
    "\n",
    "for ticker, ten_qs in tqdm(ten_q_forms.items(), total=len(ten_q_forms.keys())):\n",
    "    lemma_docs = [' '.join(ten_q) for ten_q in ten_qs]\n",
    "    \n",
    "    sentiment_bow_ten_qs[ticker] = {\n",
    "        sentiment: get_bag_of_words(\n",
    "            sentiment_df[sentiment_df[sentiment]!=0]['word'], \n",
    "            lemma_docs\n",
    "        )\n",
    "        for sentiment in sentiments\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0936b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 5/5 [00:00<00:00, 400.49it/s]\n"
     ]
    }
   ],
   "source": [
    "mag_bow = lambda bow: np.linalg.norm(bow, ord=2, axis=1)/bow.shape[1]\n",
    "\n",
    "bow_ten_qs = {}\n",
    "for ticker, sentiments in tqdm(sentiment_bow_ten_qs.items(), total=5):\n",
    "    dates = ten_q_forms_dates[ticker]\n",
    "    bow_ten_qs[ticker] = {\n",
    "        sentiment: {\n",
    "            date: mag_bow(bow)[date_idx] \n",
    "            for date_idx, date in enumerate(dates)\n",
    "        } for sentiment, bow in sentiments.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c09984",
   "metadata": {},
   "source": [
    "## 10K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75903507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [01:45<00:00, 21.09s/it]\n"
     ]
    }
   ],
   "source": [
    "sentiment_bow_ten_ks = {}\n",
    "\n",
    "for ticker, ten_ks in tqdm(ten_k_forms.items(), total=len(ten_k_forms.keys())):\n",
    "    lemma_docs = [' '.join(ten_k) for ten_k in ten_ks]\n",
    "    \n",
    "    sentiment_bow_ten_ks[ticker] = {\n",
    "        sentiment: get_bag_of_words(\n",
    "            sentiment_df[sentiment_df[sentiment]!=0]['word'], \n",
    "            lemma_docs\n",
    "        )\n",
    "        for sentiment in sentiments\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6660c927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 5/5 [00:00<00:00, 1407.67it/s]\n"
     ]
    }
   ],
   "source": [
    "mag_bow = lambda bow: np.linalg.norm(bow, ord=2, axis=1)/bow.shape[1]\n",
    "\n",
    "bow_ten_ks = {}\n",
    "for ticker, sentiments in tqdm(sentiment_bow_ten_ks.items(), total=5):\n",
    "    dates = ten_k_forms_dates[ticker]\n",
    "    bow_ten_ks[ticker] = {\n",
    "        sentiment: {\n",
    "            date: mag_bow(bow)[date_idx] \n",
    "            for date_idx, date in enumerate(dates)\n",
    "        } for sentiment, bow in sentiments.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0db6988",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa72801",
   "metadata": {},
   "source": [
    "## 10Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e81ece16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [02:50<00:00, 34.12s/it]\n"
     ]
    }
   ],
   "source": [
    "sentiment_tfidf_ten_qs = {}\n",
    "\n",
    "for ticker, ten_qs in tqdm(ten_q_forms.items(), total=len(ten_q_forms.keys())):\n",
    "    lemma_docs = [' '.join(ten_q) for ten_q in ten_qs]\n",
    "    \n",
    "    sentiment_tfidf_ten_qs[ticker] = {\n",
    "        sentiment: get_tfidf(\n",
    "            sentiment_df[sentiment_df[sentiment]!=0]['word'], \n",
    "            lemma_docs\n",
    "        )\n",
    "        for sentiment in sentiments\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca060df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 5/5 [00:00<00:00, 566.77it/s]\n"
     ]
    }
   ],
   "source": [
    "mag_tfidf = lambda tfidf: np.linalg.norm(tfidf, ord=2, axis=1)/tfidf.shape[1]\n",
    "\n",
    "tfidf_ten_qs = {}\n",
    "for ticker, sentiments in tqdm(sentiment_tfidf_ten_qs.items(), total=5):\n",
    "    dates = ten_q_forms_dates[ticker]\n",
    "    tfidf_ten_qs[ticker] = {\n",
    "        sentiment: {\n",
    "            date: mag_tfidf(tfidf)[date_idx] \n",
    "            for date_idx, date in enumerate(dates)\n",
    "        } for sentiment, tfidf in sentiments.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165a21ba",
   "metadata": {},
   "source": [
    "## 10K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00e2b593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [01:46<00:00, 21.29s/it]\n"
     ]
    }
   ],
   "source": [
    "sentiment_tfidf_ten_ks = {}\n",
    "\n",
    "for ticker, ten_ks in tqdm(ten_k_forms.items(), total=len(ten_k_forms.keys())):\n",
    "    lemma_docs = [' '.join(ten_k) for ten_k in ten_ks]\n",
    "    \n",
    "    sentiment_tfidf_ten_ks[ticker] = {\n",
    "        sentiment: get_tfidf(\n",
    "            sentiment_df[sentiment_df[sentiment]!=0]['word'], \n",
    "            lemma_docs\n",
    "        )\n",
    "        for sentiment in sentiments\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8098a942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 5/5 [00:00<00:00, 1664.54it/s]\n"
     ]
    }
   ],
   "source": [
    "mag_tfidf = lambda tfidf: np.linalg.norm(tfidf, ord=2, axis=1)/tfidf.shape[1]\n",
    "\n",
    "tfidf_ten_ks = {}\n",
    "for ticker, sentiments in tqdm(sentiment_tfidf_ten_ks.items(), total=5):\n",
    "    dates = ten_k_forms_dates[ticker]\n",
    "    tfidf_ten_ks[ticker] = {\n",
    "        sentiment: {\n",
    "            date: mag_tfidf(tfidf)[date_idx] \n",
    "            for date_idx, date in enumerate(dates)\n",
    "        } for sentiment, tfidf in sentiments.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cb761a",
   "metadata": {},
   "source": [
    "# Downloading all dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8531cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentiment_bow_ten_qs.pickle', 'wb') as handle:\n",
    "    pickle.dump(sentiment_bow_ten_qs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "with open('bow_ten_qs.pickle', 'wb') as handle:\n",
    "    pickle.dump(bow_ten_qs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "################################################################################################\n",
    "################################################################################################\n",
    "\n",
    "with open('sentiment_bow_ten_ks.pickle', 'wb') as handle:\n",
    "    pickle.dump(sentiment_bow_ten_ks, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "with open('bow_ten_ks.pickle', 'wb') as handle:\n",
    "    pickle.dump(bow_ten_ks, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "################################################################################################\n",
    "################################################################################################\n",
    "################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "with open('sentiment_tfidf_ten_qs.pickle', 'wb') as handle:\n",
    "    pickle.dump(sentiment_tfidf_ten_qs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "with open('tfidf_ten_qs.pickle', 'wb') as handle:\n",
    "    pickle.dump(tfidf_ten_qs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "################################################################################################\n",
    "################################################################################################\n",
    "\n",
    "with open('sentiment_tfidf_ten_ks.pickle', 'wb') as handle:\n",
    "    pickle.dump(sentiment_tfidf_ten_ks, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "with open('tfidf_ten_ks.pickle', 'wb') as handle:\n",
    "    pickle.dump(tfidf_ten_ks, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
